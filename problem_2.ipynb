{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import itertools\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urlretrieve(url + filename, filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified %s' % filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception(\n",
    "          'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        name = f.namelist()[0]\n",
    "        data = tf.compat.as_str(f.read(name))\n",
    "    return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "letters = sorted(set((string.ascii_letters + ' ').lower()))\n",
    "translate_table = {ord(l): ord(l) for l in letters}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigrams = {c1 + c2: i for i, (c1, c2) in \n",
    "           enumerate(itertools.product(letters, letters))}\n",
    "inversed_bigrams = {v: k for k, v in bigrams.items()}\n",
    "\n",
    "vocabulary_size = len(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def id_2_bigram(x):\n",
    "    return inversed_bigrams[x]\n",
    "\n",
    "def text_2_bigrams(text):\n",
    "    text = text.translate(translate_table)\n",
    "    if len(text) % 2 != 0:\n",
    "        text += ' '\n",
    "    return np.array([bigrams[text[i:i + 2]] for i in range(0, len(text), 2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BatchGenerator(object):\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "\n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "        batch = np.zeros(\n",
    "            shape=(self._batch_size), dtype=np.int16)\n",
    "        for b in range(self._batch_size):\n",
    "            batch[b] = self._text[self._cursor[b]]\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "        return batch\n",
    "\n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "    \n",
    "class BigramBatchGenerator(object):\n",
    "    def __init__(self, text, batch_size, num_unrollings=0):\n",
    "        bigrams = text_2_bigrams(text)\n",
    "        self._generator = BatchGenerator(bigrams, batch_size, num_unrollings)\n",
    "\n",
    "    def next(self):\n",
    "        return self._generator.next()\n",
    "    \n",
    "def characters(bigram_ids):\n",
    "    return [id_2_bigram(b_id) for b_id in bigram_ids]\n",
    "\n",
    "def batches2string(batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "    representation.\"\"\"\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, characters(b))]\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchists advocat', 'when military governme', 'lleria arches national', ' abbeys and monasterie', 'married urraca princes', 'hel and richard baer h', 'y and liturgical langu', 'ay opened for passenge', 'tion from the national', 'migration took place d', 'new york other well kn', 'he boeing seven six se', 'e listed with a gloss ', 'eber has probably been', 'o be made to recognize', 'yer who received the f', 'ore significant than i', 'a fierce critic of the', ' two six eight in sign', 'aristotle s uncaused c', 'ity can be lost as in ', ' and intracellular ice', 'tion of the size of th', 'dy to pass him a stick', 'f certain drugs confus', 'at it will take to com', 'e convince the priest ', 'ent told him to name i', 'ampaign and barred att', 'rver side standard for', 'ious texts such as eso', 'o capitalize on the gr', 'a duplicate of the ori', 'gh ann es d hiver one ', 'ine january eight marc', 'ross zero the lead cha', 'cal theories classical', 'ast instance the non g', ' dimensional analysis ', 'most holy mormons beli', 't s support or at leas', 'u is still disagreed u', 'e oscillating system e', 'o eight subtypes based', 'of italy languages the', 's the tower commission', 'klahoma press one nine', 'erprise linux suse lin', 'ws becomes the first d', 'et in a nazi concentra', 'the fabian society neh', 'etchy to relatively st', ' sharman networks shar', 'ised emperor hirohito ', 'ting in political init', 'd neo latin most of th', 'th risky riskerdoo ric', 'encyclopedic overview ', 'fense the air componen', 'duating from acnm accr', 'treet grid centerline ', 'ations more than any o', 'appeal of devotional b', 'si have made such devi']\n",
      "['ate social relations b', 'ments failed to revive', 'al park photographic v', 'ies index sacred desti', 'ess of castile daughte', ' h provided a detailed', 'guage among jews manda', 'gers in december one n', 'al media and from pres', ' during the one nine e', 'known manufacturers of', 'seven a widebody jet w', 's covering some of the', 'en one of the most inf', 'ze single acts of meri', ' first card from the d', ' in jersey and guernse', 'he poverty and social ', 'gns of humanity vol th', ' cause so aquinas come', 'n denaturalization and', 'ce formation solution ', 'the input usually meas', 'ck to pull him out but', 'usion inability to ori', 'omplete an operation c', 't of the mistakes of a', ' it fort des moines th', 'ttempts by his opponen', 'ormats for mailboxes i', 'soteric christianity a', 'growing popularity of ', 'riginal document fax m', 'e nine eight zero one ', 'rch eight listing of a', 'haracter lieutenant sh', 'al mechanics and speci', ' gm comparison maize c', 's fundamental applicat', 'lieve the configuratio', 'ast not parliament s o', ' upon by historians an', ' example rlc circuit f', 'ed on the whole genome', 'he official language o', 'on at this point presi', 'ne three two one one t', 'inux enterprise server', ' daily college newspap', 'ration camp lewis has ', 'ehru wished the econom', 'stiff from flat to tig', 'arman s sydney based b', 'o to begin negotiation', 'itiatives the lesotho ', 'these authors wrote in', 'icky ricardo this clas', 'w of mathematics prese', 'ent of arm is represen', 'credited programs must', 'e external links bbc o', ' other state modern da', ' buddhism especially r', 'vices possible the sys']\n",
      "[' ana']\n",
      "['narc']\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "num_unrollings = 10\n",
    "\n",
    "train_batches = BigramBatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BigramBatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 256\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1, 1))\n",
    "    \n",
    "    IW = tf.Variable(tf.truncated_normal([embedding_size, num_nodes * 4], -0.1, 0.1))\n",
    "    OW = tf.Variable(tf.truncated_normal([num_nodes, num_nodes * 4], -0.1, 0.1))\n",
    "    B = tf.Variable(tf.zeros([1, num_nodes * 4]))\n",
    "    \n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "    def lstm_cell(i, o, state):\n",
    "        gate = tf.matmul(i, IW) + tf.matmul(o, OW) + B\n",
    "        input_gate = tf.sigmoid(gate[:, 0:num_nodes])\n",
    "        forget_gate = tf.sigmoid(gate[:, num_nodes:2 * num_nodes])\n",
    "        update = tf.tanh(gate[:, 2 * num_nodes:3 * num_nodes])\n",
    "        output_gate = tf.sigmoid(gate[:, 3 * num_nodes:])\n",
    "        state = forget_gate * state + input_gate * update\n",
    "        return output_gate * tf.tanh(state), state\n",
    "    \n",
    "    train_data = []\n",
    "    embedded_train_data = []\n",
    "    \n",
    "    for _ in range(num_unrollings + 1):\n",
    "        batch = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "        train_data.append(batch)\n",
    "        embedded_batch = tf.nn.embedding_lookup(embeddings, batch)\n",
    "        embedded_train_data.append(embedded_batch)\n",
    "        \n",
    "    train_inputs = embedded_train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]\n",
    "\n",
    "    outputs = []\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    \n",
    "    for i in train_inputs:\n",
    "        i = tf.nn.dropout(i, keep_prob)\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        output = tf.nn.dropout(output, keep_prob)\n",
    "        outputs.append(output)\n",
    "        \n",
    "    \n",
    "    with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        labels = tf.one_hot(tf.concat(train_labels, 0), vocabulary_size)\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits,\n",
    "                                                                      labels=labels))\n",
    "    \n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "    \n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    sample_input = tf.placeholder(tf.int32, shape=[1])\n",
    "    embedded_sample_input = tf.nn.embedding_lookup(embeddings, sample_input)\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    \n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes]))\n",
    "    )\n",
    "    \n",
    "    sample_output, sample_state = lstm_cell(embedded_sample_input,\n",
    "                                            saved_sample_output,\n",
    "                                            saved_sample_state)\n",
    "    \n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                  saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(tf.concat(sample_output, 0), w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(distribution):\n",
    "    \"\"\"Sample one element from a distribution assumed\n",
    "    to be an array of normalized probabilities.\n",
    "    \"\"\"\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "def logprob(predictions, labels):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot(x, n_classes=vocabulary_size):\n",
    "    X = np.zeros((len(x), n_classes))\n",
    "    X[np.arange(len(x)), x] = 1\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 6.601061 learning rate: 10.000000\n",
      "Minibatch perplexity: 735.88\n",
      "================================================================================\n",
      "axwongbgwmsefteiqnbjumywamhtkcdigllx g uhm\n",
      "pjbl sr gnipuzdbbuspkabqmyydxkzrrbiggqumey\n",
      "pqhcuf icgepyhrksdpqordickhzzhyveggzjmgimn\n",
      "ryfrdxgnoivavnvaefykltobapywqqyddsnvkpkiwu\n",
      " ywfciottkmumewol nwuiton exearxxpmu rkmsi\n",
      "================================================================================\n",
      "Validation set perplexity: 658.60\n",
      "Average loss at step 200: 4.610123 learning rate: 10.000000\n",
      "Minibatch perplexity: 62.37\n",
      "Validation set perplexity: 54.67\n",
      "Average loss at step 400: 3.982599 learning rate: 10.000000\n",
      "Minibatch perplexity: 49.98\n",
      "Validation set perplexity: 42.36\n",
      "Average loss at step 600: 3.871839 learning rate: 10.000000\n",
      "Minibatch perplexity: 53.76\n",
      "Validation set perplexity: 33.81\n",
      "Average loss at step 800: 3.801738 learning rate: 10.000000\n",
      "Minibatch perplexity: 43.68\n",
      "Validation set perplexity: 33.15\n",
      "Average loss at step 1000: 3.713014 learning rate: 10.000000\n",
      "Minibatch perplexity: 43.28\n",
      "Validation set perplexity: 30.54\n",
      "Average loss at step 1200: 3.716199 learning rate: 10.000000\n",
      "Minibatch perplexity: 48.45\n",
      "Validation set perplexity: 27.26\n",
      "Average loss at step 1400: 3.709326 learning rate: 10.000000\n",
      "Minibatch perplexity: 34.23\n",
      "Validation set perplexity: 26.09\n",
      "Average loss at step 1600: 3.673637 learning rate: 10.000000\n",
      "Minibatch perplexity: 32.61\n",
      "Validation set perplexity: 25.90\n",
      "Average loss at step 1800: 3.699930 learning rate: 10.000000\n",
      "Minibatch perplexity: 34.86\n",
      "Validation set perplexity: 25.27\n",
      "Average loss at step 2000: 3.679228 learning rate: 10.000000\n",
      "Minibatch perplexity: 38.52\n",
      "================================================================================\n",
      "zjid councrome forrul rutcess eight one ni\n",
      "words profy impo with been resarary in has\n",
      "avate of resrenthill gromon of his pohr af\n",
      "xceconculetch imotmxually in aping or lati\n",
      "pj and oshs partme south with his repleaza\n",
      "================================================================================\n",
      "Validation set perplexity: 26.09\n",
      "Average loss at step 2200: 3.643968 learning rate: 10.000000\n",
      "Minibatch perplexity: 31.02\n",
      "Validation set perplexity: 25.03\n",
      "Average loss at step 2400: 3.647865 learning rate: 10.000000\n",
      "Minibatch perplexity: 40.44\n",
      "Validation set perplexity: 24.68\n",
      "Average loss at step 2600: 3.628945 learning rate: 10.000000\n",
      "Minibatch perplexity: 37.00\n",
      "Validation set perplexity: 24.82\n",
      "Average loss at step 2800: 3.588359 learning rate: 10.000000\n",
      "Minibatch perplexity: 35.92\n",
      "Validation set perplexity: 24.88\n",
      "Average loss at step 3000: 3.578826 learning rate: 10.000000\n",
      "Minibatch perplexity: 35.12\n",
      "Validation set perplexity: 25.30\n",
      "Average loss at step 3200: 3.524679 learning rate: 10.000000\n",
      "Minibatch perplexity: 33.75\n",
      "Validation set perplexity: 23.97\n",
      "Average loss at step 3400: 3.592186 learning rate: 10.000000\n",
      "Minibatch perplexity: 36.87\n",
      "Validation set perplexity: 23.85\n",
      "Average loss at step 3600: 3.566249 learning rate: 10.000000\n",
      "Minibatch perplexity: 36.21\n",
      "Validation set perplexity: 24.15\n",
      "Average loss at step 3800: 3.571783 learning rate: 10.000000\n",
      "Minibatch perplexity: 30.52\n",
      "Validation set perplexity: 24.53\n",
      "Average loss at step 4000: 3.595196 learning rate: 10.000000\n",
      "Minibatch perplexity: 44.58\n",
      "================================================================================\n",
      "o was a world attrb the them his to be one\n",
      "ad lown sther of s wide the would it cstin\n",
      "esribracitlatic acce the unieventa one sev\n",
      "kx one nine nine zero five atter four is f\n",
      "nnrevoly arly works in the his nession two\n",
      "================================================================================\n",
      "Validation set perplexity: 24.52\n",
      "Average loss at step 4200: 3.577648 learning rate: 10.000000\n",
      "Minibatch perplexity: 47.98\n",
      "Validation set perplexity: 24.03\n",
      "Average loss at step 4400: 3.554617 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.16\n",
      "Validation set perplexity: 22.92\n",
      "Average loss at step 4600: 3.554378 learning rate: 10.000000\n",
      "Minibatch perplexity: 33.51\n",
      "Validation set perplexity: 23.18\n",
      "Average loss at step 4800: 3.589399 learning rate: 10.000000\n",
      "Minibatch perplexity: 33.21\n",
      "Validation set perplexity: 23.56\n",
      "Average loss at step 5000: 3.596312 learning rate: 1.000000\n",
      "Minibatch perplexity: 32.29\n",
      "Validation set perplexity: 23.62\n",
      "Average loss at step 5200: 3.545533 learning rate: 1.000000\n",
      "Minibatch perplexity: 31.55\n",
      "Validation set perplexity: 23.03\n",
      "Average loss at step 5400: 3.591917 learning rate: 1.000000\n",
      "Minibatch perplexity: 37.87\n",
      "Validation set perplexity: 22.74\n",
      "Average loss at step 5600: 3.549713 learning rate: 1.000000\n",
      "Minibatch perplexity: 36.78\n",
      "Validation set perplexity: 22.54\n",
      "Average loss at step 5800: 3.550329 learning rate: 1.000000\n",
      "Minibatch perplexity: 44.63\n",
      "Validation set perplexity: 22.16\n",
      "Average loss at step 6000: 3.545603 learning rate: 1.000000\n",
      "Minibatch perplexity: 29.85\n",
      "================================================================================\n",
      "ove their metrochgative in a ar the x five\n",
      "e five one nine eight one eight nine seven\n",
      "mctal ure one nine four four nine thwithwe\n",
      "tqges many recame of densy which and frequ\n",
      "qflion of subse desclacte commow due his g\n",
      "================================================================================\n",
      "Validation set perplexity: 22.03\n",
      "Average loss at step 6200: 3.538920 learning rate: 1.000000\n",
      "Minibatch perplexity: 36.57\n",
      "Validation set perplexity: 22.04\n",
      "Average loss at step 6400: 3.519581 learning rate: 1.000000\n",
      "Minibatch perplexity: 33.66\n",
      "Validation set perplexity: 21.87\n",
      "Average loss at step 6600: 3.520269 learning rate: 1.000000\n",
      "Minibatch perplexity: 36.59\n",
      "Validation set perplexity: 21.93\n",
      "Average loss at step 6800: 3.521636 learning rate: 1.000000\n",
      "Minibatch perplexity: 41.17\n",
      "Validation set perplexity: 21.90\n",
      "Average loss at step 7000: 3.514072 learning rate: 1.000000\n",
      "Minibatch perplexity: 34.38\n",
      "Validation set perplexity: 22.02\n",
      "Average loss at step 7200: 3.491465 learning rate: 1.000000\n",
      "Minibatch perplexity: 28.26\n",
      "Validation set perplexity: 21.92\n",
      "Average loss at step 7400: 3.537406 learning rate: 1.000000\n",
      "Minibatch perplexity: 34.89\n",
      "Validation set perplexity: 21.99\n",
      "Average loss at step 7600: 3.500389 learning rate: 1.000000\n",
      "Minibatch perplexity: 38.33\n",
      "Validation set perplexity: 21.90\n",
      "Average loss at step 7800: 3.483934 learning rate: 1.000000\n",
      "Minibatch perplexity: 32.21\n",
      "Validation set perplexity: 21.72\n",
      "Average loss at step 8000: 3.529484 learning rate: 1.000000\n",
      "Minibatch perplexity: 34.70\n",
      "================================================================================\n",
      "nnetlonguagea mospesty uses which toian pr\n",
      "jiby sremart for teers viewasdols for a co\n",
      "wbegist diffong mav marcion ands dayabs in\n",
      "uhs world high spemened use widwial afking\n",
      "tdven one zero five zero seven jypcheats d\n",
      "================================================================================\n",
      "Validation set perplexity: 21.68\n",
      "Average loss at step 8200: 3.506402 learning rate: 1.000000\n",
      "Minibatch perplexity: 40.56\n",
      "Validation set perplexity: 21.69\n",
      "Average loss at step 8400: 3.532843 learning rate: 1.000000\n",
      "Minibatch perplexity: 31.20\n",
      "Validation set perplexity: 21.73\n",
      "Average loss at step 8600: 3.520019 learning rate: 1.000000\n",
      "Minibatch perplexity: 33.14\n",
      "Validation set perplexity: 21.37\n",
      "Average loss at step 8800: 3.497569 learning rate: 1.000000\n",
      "Minibatch perplexity: 40.38\n",
      "Validation set perplexity: 21.43\n",
      "Average loss at step 9000: 3.494297 learning rate: 1.000000\n",
      "Minibatch perplexity: 27.28\n",
      "Validation set perplexity: 21.45\n",
      "Average loss at step 9200: 3.487924 learning rate: 1.000000\n",
      "Minibatch perplexity: 37.30\n",
      "Validation set perplexity: 21.55\n",
      "Average loss at step 9400: 3.508288 learning rate: 1.000000\n",
      "Minibatch perplexity: 35.60\n",
      "Validation set perplexity: 21.54\n",
      "Average loss at step 9600: 3.548706 learning rate: 1.000000\n",
      "Minibatch perplexity: 30.84\n",
      "Validation set perplexity: 21.50\n",
      "Average loss at step 9800: 3.526543 learning rate: 1.000000\n",
      "Minibatch perplexity: 39.68\n",
      "Validation set perplexity: 21.47\n",
      "Average loss at step 10000: 3.487112 learning rate: 0.100000\n",
      "Minibatch perplexity: 33.26\n",
      "================================================================================\n",
      "han unanth julary as with an lelled bit to\n",
      "yrard in danthe in one was sover a neans c\n",
      "zhulary compross of alled on book to ef re\n",
      "wvast two zero zero six cliporphics massmo\n",
      " of the desctiation of the r commall s a s\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 21.65\n",
      "Average loss at step 10200: 3.550837 learning rate: 0.100000\n",
      "Minibatch perplexity: 37.38\n",
      "Validation set perplexity: 21.63\n",
      "Average loss at step 10400: 3.540404 learning rate: 0.100000\n",
      "Minibatch perplexity: 29.73\n",
      "Validation set perplexity: 21.58\n",
      "Average loss at step 10600: 3.488560 learning rate: 0.100000\n",
      "Minibatch perplexity: 38.01\n",
      "Validation set perplexity: 21.55\n",
      "Average loss at step 10800: 3.511946 learning rate: 0.100000\n",
      "Minibatch perplexity: 39.60\n",
      "Validation set perplexity: 21.50\n",
      "Average loss at step 11000: 3.502890 learning rate: 0.100000\n",
      "Minibatch perplexity: 26.98\n",
      "Validation set perplexity: 21.47\n",
      "Average loss at step 11200: 3.516047 learning rate: 0.100000\n",
      "Minibatch perplexity: 36.30\n",
      "Validation set perplexity: 21.39\n",
      "Average loss at step 11400: 3.510704 learning rate: 0.100000\n",
      "Minibatch perplexity: 34.48\n",
      "Validation set perplexity: 21.30\n",
      "Average loss at step 11600: 3.532881 learning rate: 0.100000\n",
      "Minibatch perplexity: 44.94\n",
      "Validation set perplexity: 21.26\n",
      "Average loss at step 11800: 3.532566 learning rate: 0.100000\n",
      "Minibatch perplexity: 32.16\n",
      "Validation set perplexity: 21.22\n",
      "Average loss at step 12000: 3.537832 learning rate: 0.100000\n",
      "Minibatch perplexity: 40.41\n",
      "================================================================================\n",
      "qmly s the s specipare impose and conhing \n",
      "vx gional goilitie as meth exterclrek of i\n",
      "ourole pokhy modern the misuing and scitue\n",
      "js a to as abur also the fibarl akin his t\n",
      "sand a serent and lodas for shbo aqsran al\n",
      "================================================================================\n",
      "Validation set perplexity: 21.20\n",
      "Average loss at step 12200: 3.512559 learning rate: 0.100000\n",
      "Minibatch perplexity: 35.81\n",
      "Validation set perplexity: 21.15\n",
      "Average loss at step 12400: 3.491268 learning rate: 0.100000\n",
      "Minibatch perplexity: 30.53\n",
      "Validation set perplexity: 21.12\n",
      "Average loss at step 12600: 3.498991 learning rate: 0.100000\n",
      "Minibatch perplexity: 32.34\n",
      "Validation set perplexity: 21.12\n",
      "Average loss at step 12800: 3.464524 learning rate: 0.100000\n",
      "Minibatch perplexity: 32.96\n",
      "Validation set perplexity: 21.03\n",
      "Average loss at step 13000: 3.525458 learning rate: 0.100000\n",
      "Minibatch perplexity: 30.32\n",
      "Validation set perplexity: 21.04\n",
      "Average loss at step 13200: 3.499357 learning rate: 0.100000\n",
      "Minibatch perplexity: 26.88\n",
      "Validation set perplexity: 21.02\n",
      "Average loss at step 13400: 3.483316 learning rate: 0.100000\n",
      "Minibatch perplexity: 31.91\n",
      "Validation set perplexity: 21.00\n",
      "Average loss at step 13600: 3.496945 learning rate: 0.100000\n",
      "Minibatch perplexity: 41.28\n",
      "Validation set perplexity: 21.07\n",
      "Average loss at step 13800: 3.513649 learning rate: 0.100000\n",
      "Minibatch perplexity: 38.36\n",
      "Validation set perplexity: 21.11\n",
      "Average loss at step 14000: 3.542057 learning rate: 0.100000\n",
      "Minibatch perplexity: 33.99\n",
      "================================================================================\n",
      "bqe to sence from dahh remaining comals ta\n",
      "zdts approvands solightity forvoph was s m\n",
      "sgllemen also hint stinds that to string s\n",
      "yfus the mulicattaliting greet si as was w\n",
      "mber sbsgition to the persrip ginin scient\n",
      "================================================================================\n",
      "Validation set perplexity: 21.15\n",
      "Average loss at step 14200: 3.514651 learning rate: 0.100000\n",
      "Minibatch perplexity: 41.24\n",
      "Validation set perplexity: 21.12\n",
      "Average loss at step 14400: 3.512098 learning rate: 0.100000\n",
      "Minibatch perplexity: 41.63\n",
      "Validation set perplexity: 21.13\n",
      "Average loss at step 14600: 3.507510 learning rate: 0.100000\n",
      "Minibatch perplexity: 35.26\n",
      "Validation set perplexity: 21.15\n",
      "Average loss at step 14800: 3.505886 learning rate: 0.100000\n",
      "Minibatch perplexity: 29.49\n",
      "Validation set perplexity: 21.16\n",
      "Average loss at step 15000: 3.523113 learning rate: 0.010000\n",
      "Minibatch perplexity: 31.72\n",
      "Validation set perplexity: 21.15\n",
      "Average loss at step 15200: 3.509280 learning rate: 0.010000\n",
      "Minibatch perplexity: 37.20\n",
      "Validation set perplexity: 21.15\n",
      "Average loss at step 15400: 3.478695 learning rate: 0.010000\n",
      "Minibatch perplexity: 30.35\n",
      "Validation set perplexity: 21.15\n",
      "Average loss at step 15600: 3.482063 learning rate: 0.010000\n",
      "Minibatch perplexity: 33.91\n",
      "Validation set perplexity: 21.16\n",
      "Average loss at step 15800: 3.483693 learning rate: 0.010000\n",
      "Minibatch perplexity: 34.91\n",
      "Validation set perplexity: 21.16\n",
      "Average loss at step 16000: 3.504814 learning rate: 0.010000\n",
      "Minibatch perplexity: 43.58\n",
      "================================================================================\n",
      "eenen johneo one zero zero zero zero eight\n",
      "h writed as shows to bestar words at infli\n",
      "vue john nic naturing one massian a crown \n",
      "yas which is istlogoncing wealliented mepo\n",
      "iwun in scotght memt to whusic to boides o\n",
      "================================================================================\n",
      "Validation set perplexity: 21.15\n",
      "Average loss at step 16200: 3.504855 learning rate: 0.010000\n",
      "Minibatch perplexity: 34.54\n",
      "Validation set perplexity: 21.15\n",
      "Average loss at step 16400: 3.502560 learning rate: 0.010000\n",
      "Minibatch perplexity: 31.74\n",
      "Validation set perplexity: 21.14\n",
      "Average loss at step 16600: 3.511634 learning rate: 0.010000\n",
      "Minibatch perplexity: 36.26\n",
      "Validation set perplexity: 21.13\n",
      "Average loss at step 16800: 3.500844 learning rate: 0.010000\n",
      "Minibatch perplexity: 38.09\n",
      "Validation set perplexity: 21.13\n",
      "Average loss at step 17000: 3.485288 learning rate: 0.010000\n",
      "Minibatch perplexity: 29.15\n",
      "Validation set perplexity: 21.13\n",
      "Average loss at step 17200: 3.528327 learning rate: 0.010000\n",
      "Minibatch perplexity: 39.43\n",
      "Validation set perplexity: 21.12\n",
      "Average loss at step 17400: 3.535860 learning rate: 0.010000\n",
      "Minibatch perplexity: 29.44\n",
      "Validation set perplexity: 21.13\n",
      "Average loss at step 17600: 3.498255 learning rate: 0.010000\n",
      "Minibatch perplexity: 35.06\n",
      "Validation set perplexity: 21.12\n",
      "Average loss at step 17800: 3.515932 learning rate: 0.010000\n",
      "Minibatch perplexity: 38.79\n",
      "Validation set perplexity: 21.11\n",
      "Average loss at step 18000: 3.544402 learning rate: 0.010000\n",
      "Minibatch perplexity: 41.69\n",
      "================================================================================\n",
      "sva margently and dedia in one four mison \n",
      "yz reatorion two zero zero two zero eight \n",
      "bsseven three s le many or typents to it o\n",
      "v adcial one seven eight seven eight one f\n",
      "u the city one eight k vatter by destres t\n",
      "================================================================================\n",
      "Validation set perplexity: 21.10\n",
      "Average loss at step 18200: 3.513939 learning rate: 0.010000\n",
      "Minibatch perplexity: 38.65\n",
      "Validation set perplexity: 21.09\n",
      "Average loss at step 18400: 3.542456 learning rate: 0.010000\n",
      "Minibatch perplexity: 36.22\n",
      "Validation set perplexity: 21.09\n",
      "Average loss at step 18600: 3.541522 learning rate: 0.010000\n",
      "Minibatch perplexity: 31.66\n",
      "Validation set perplexity: 21.08\n",
      "Average loss at step 18800: 3.546674 learning rate: 0.010000\n",
      "Minibatch perplexity: 33.48\n",
      "Validation set perplexity: 21.08\n",
      "Average loss at step 19000: 3.553607 learning rate: 0.010000\n",
      "Minibatch perplexity: 34.77\n",
      "Validation set perplexity: 21.07\n",
      "Average loss at step 19200: 3.564438 learning rate: 0.010000\n",
      "Minibatch perplexity: 38.83\n",
      "Validation set perplexity: 21.06\n",
      "Average loss at step 19400: 3.530413 learning rate: 0.010000\n",
      "Minibatch perplexity: 34.39\n",
      "Validation set perplexity: 21.05\n",
      "Average loss at step 19600: 3.505868 learning rate: 0.010000\n",
      "Minibatch perplexity: 34.35\n",
      "Validation set perplexity: 21.05\n",
      "Average loss at step 19800: 3.471480 learning rate: 0.010000\n",
      "Minibatch perplexity: 28.32\n",
      "Validation set perplexity: 21.05\n",
      "Average loss at step 20000: 3.453627 learning rate: 0.001000\n",
      "Minibatch perplexity: 34.65\n",
      "================================================================================\n",
      "uvlirtan on and tift times used leoo servi\n",
      "uxling a diventuhat paletion to directions\n",
      "dnatesr and s nvactures a b one jenated th\n",
      "if qd her deseves visish ecozp romalno wer\n",
      "qytes with commoke defint difar two six on\n",
      "================================================================================\n",
      "Validation set perplexity: 21.04\n"
     ]
    }
   ],
   "source": [
    "train_stat = []\n",
    "\n",
    "num_steps = 20001\n",
    "summary_frequency = 200\n",
    "keep_prob_ = 0.7\n",
    "\n",
    "# gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.5)\n",
    "# config = tf.ConfigProto(gpu_options=gpu_options, log_device_placement=True)\n",
    "config = None\n",
    "\n",
    "with tf.Session(graph=graph, config=config) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = {train_data_i: batch.reshape((batch_size,)) \n",
    "                     for train_data_i, batch in zip(train_data, batches)}\n",
    "        feed_dict[keep_prob] = keep_prob_\n",
    "    \n",
    "        _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], \n",
    "                                            feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss /= summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            train_pp = float(np.exp(logprob(predictions, one_hot(labels))))\n",
    "            print('Minibatch perplexity: %.2f' % train_pp)\n",
    "    \n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "\n",
    "                for _ in range(5):\n",
    "                    # sample text\n",
    "                    bigram_id = np.random.randint(0, vocabulary_size) # initial bigram id\n",
    "                    sample_len = 40\n",
    "\n",
    "\n",
    "                    text = id_2_bigram(bigram_id)\n",
    "\n",
    "                    for _ in range(sample_len // 2):\n",
    "                        prediction = sample_prediction.eval({sample_input: [bigram_id], keep_prob: 1.0})\n",
    "                        bigram_id = sample(prediction[0])\n",
    "                        text += id_2_bigram(bigram_id)\n",
    "\n",
    "                    print(text)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0], keep_prob: 1.0})\n",
    "                valid_logprob += logprob(predictions, one_hot(b[1]))\n",
    "                \n",
    "            valid_pp = float(np.exp(valid_logprob / valid_size))\n",
    "            print('Validation set perplexity: %.2f' % valid_pp)\n",
    "            \n",
    "            train_stat.append({\n",
    "                'step': step,\n",
    "                'train_pp': train_pp,\n",
    "                'valid_pp': valid_pp\n",
    "            })"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
