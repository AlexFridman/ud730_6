{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import itertools\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urlretrieve(url + filename, filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified %s' % filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception(\n",
    "          'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        name = f.namelist()[0]\n",
    "        data = tf.compat.as_str(f.read(name))\n",
    "    return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PAD = 0\n",
    "EOS = 1\n",
    "\n",
    "letters = sorted(set((string.ascii_letters + ' ').lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 a\n",
      "28 z\n",
      "2  \n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "2  \n",
      "0 P\n",
      "1 E\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(letters) + 2 # [a-z] + ' ' + PAD + EOS\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "    if char in [PAD, EOS]:\n",
    "        return char\n",
    "    elif char in string.ascii_lowercase:\n",
    "        return ord(char) - first_letter + 3\n",
    "    elif char == ' ':\n",
    "        return 2\n",
    "    else:\n",
    "        print('Unexpected character: %s' % char)\n",
    "        return 2\n",
    "\n",
    "def id2char(dictid):\n",
    "    if dictid > 2:\n",
    "        return chr(dictid + first_letter - 3)\n",
    "    elif dictid == 2:\n",
    "        return ' '\n",
    "    else:\n",
    "        return {PAD: 'P', EOS: 'E'}[dictid]\n",
    "\n",
    "chars = ['a', 'z', ' ', 'ï', PAD, EOS]\n",
    "\n",
    "for char in chars:\n",
    "    print(char2id(char), id2char(char2id(char)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45000000  house listeners were addressed during a part of several success\n",
      "5000000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = int(len(text) * 0.1 / 2)\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:valid_size*10]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])\n",
    "\n",
    "class BatchGenerator(object):\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [offset * segment for offset in range(batch_size)]\n",
    "\n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "        batch = []\n",
    "        \n",
    "        for b in range(self._batch_size):\n",
    "            batch.append(char2id(self._text[self._cursor[b]]))\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "            \n",
    "        return batch\n",
    "\n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batch = [[] for _ in range(self._batch_size)]\n",
    "        for _ in range(self._num_unrollings):\n",
    "            for i, c_id in enumerate(self._next_batch()):\n",
    "                batch[i].append(c_id)\n",
    "        return batch\n",
    "\n",
    "batch_size = 64\n",
    "num_unrollings = 20\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 10, num_unrollings)\n",
    "\n",
    "def make_labels(batch):\n",
    "    import copy\n",
    "    batch = copy.deepcopy(batch)\n",
    "    for i, s in enumerate(batch):\n",
    "        s = ''.join(map(id2char, s))\n",
    "        rev_s = ' '.join(''.join(reversed(w)) for w in s.split(' '))\n",
    "        batch[i] = list(map(char2id, rev_s))\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " house listeners wer\n",
      " esuoh srenetsil rew\n"
     ]
    }
   ],
   "source": [
    "tb = train_batches.next()\n",
    "labels = make_labels(tb)\n",
    "\n",
    "print(''.join(map(id2char, tb[0])))\n",
    "print(''.join(map(id2char, labels[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_batch(inputs, max_sequence_length=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        inputs:\n",
    "            list of sentences (integer lists)\n",
    "        max_sequence_length:\n",
    "            integer specifying how large should `max_time` dimension be.\n",
    "            If None, maximum sequence length would be used\n",
    "    \n",
    "    Outputs:\n",
    "        inputs_time_major:\n",
    "            input sentences transformed into time-major matrix \n",
    "            (shape [max_time, batch_size]) padded with 0s\n",
    "        sequence_lengths:\n",
    "            batch-sized list of integers specifying amount of active \n",
    "            time steps in each input sequence\n",
    "    \"\"\"\n",
    "    \n",
    "    sequence_lengths = [len(seq) for seq in inputs]\n",
    "    batch_size = len(inputs)\n",
    "    \n",
    "    if max_sequence_length is None:\n",
    "        max_sequence_length = max(sequence_lengths)\n",
    "    \n",
    "    inputs_batch_major = np.zeros(shape=[batch_size, max_sequence_length], dtype=np.int32) # == PAD\n",
    "    \n",
    "    for i, seq in enumerate(inputs):\n",
    "        for j, element in enumerate(seq):\n",
    "            inputs_batch_major[i, j] = element\n",
    "\n",
    "    # [batch_size, max_time] -> [max_time, batch_size]\n",
    "    inputs_time_major = inputs_batch_major.swapaxes(0, 1)\n",
    "\n",
    "    return inputs_time_major, sequence_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = tf.ConfigProto(\n",
    "        device_count = {'GPU': 0}\n",
    "    )\n",
    "\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads=3, inter_op_parallelism_threads=3, \\\n",
    "                        allow_soft_placement=True, \\\n",
    "                        device_count = {'CPU': 1, 'GPU': 0})\n",
    "\n",
    "config = None\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession(config=config)\n",
    "\n",
    "input_embedding_size = 20\n",
    "\n",
    "encoder_hidden_units = 40\n",
    "decoder_hidden_units = encoder_hidden_units * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_inputs = tf.placeholder(shape=(None, None), dtype=tf.int32, name='encoder_inputs')\n",
    "encoder_inputs_length = tf.placeholder(shape=(None,), dtype=tf.int32, name='encoder_inputs_length')\n",
    "\n",
    "decoder_targets = tf.placeholder(shape=(None, None), dtype=tf.int32, name='decoder_targets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings = tf.Variable(tf.random_uniform([vocab_size, input_embedding_size], -1.0, 1.0), dtype=tf.float32)\n",
    "\n",
    "encoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, encoder_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.rnn import LSTMCell, LSTMStateTuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_cell = LSTMCell(encoder_hidden_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "((encoder_fw_outputs,\n",
    "  encoder_bw_outputs),\n",
    " (encoder_fw_final_state,\n",
    "  encoder_bw_final_state)) = (\n",
    "    tf.nn.bidirectional_dynamic_rnn(cell_fw=encoder_cell,\n",
    "                                    cell_bw=encoder_cell,\n",
    "                                    inputs=encoder_inputs_embedded,\n",
    "                                    sequence_length=encoder_inputs_length,\n",
    "                                    dtype=tf.float32, time_major=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_outputs = tf.concat((encoder_fw_outputs, encoder_bw_outputs), 2)\n",
    "\n",
    "encoder_final_state_c = tf.concat(\n",
    "    (encoder_fw_final_state.c, encoder_bw_final_state.c), 1)\n",
    "\n",
    "encoder_final_state_h = tf.concat(\n",
    "    (encoder_fw_final_state.h, encoder_bw_final_state.h), 1)\n",
    "\n",
    "encoder_final_state = LSTMStateTuple(\n",
    "    c=encoder_final_state_c,\n",
    "    h=encoder_final_state_h\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_cell = LSTMCell(decoder_hidden_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_max_time, batch_size = tf.unstack(tf.shape(encoder_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_lengths = encoder_inputs_length + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.random_uniform([decoder_hidden_units, vocab_size], -1, 1), dtype=tf.float32)\n",
    "b = tf.Variable(tf.zeros([vocab_size]), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert EOS == 1 and PAD == 0\n",
    "\n",
    "eos_time_slice = tf.ones([batch_size], dtype=tf.int32, name='EOS')\n",
    "pad_time_slice = tf.zeros([batch_size], dtype=tf.int32, name='PAD')\n",
    "\n",
    "eos_step_embedded = tf.nn.embedding_lookup(embeddings, eos_time_slice)\n",
    "pad_step_embedded = tf.nn.embedding_lookup(embeddings, pad_time_slice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loop_fn_initial():\n",
    "    initial_elements_finished = (0 >= decoder_lengths)  # all False at the initial step\n",
    "    initial_input = eos_step_embedded\n",
    "    initial_cell_state = encoder_final_state\n",
    "    initial_cell_output = None\n",
    "    initial_loop_state = None  # we don't need to pass any additional information\n",
    "    return (initial_elements_finished,\n",
    "            initial_input,\n",
    "            initial_cell_state,\n",
    "            initial_cell_output,\n",
    "            initial_loop_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loop_fn_transition(time, previous_output, previous_state, previous_loop_state):\n",
    "\n",
    "    def get_next_input():\n",
    "        output_logits = tf.add(tf.matmul(previous_output, W), b)\n",
    "        prediction = tf.argmax(output_logits, axis=1)\n",
    "        next_input = tf.nn.embedding_lookup(embeddings, prediction)\n",
    "        return next_input\n",
    "    \n",
    "    elements_finished = (time >= decoder_lengths) # this operation produces boolean tensor of [batch_size]\n",
    "                                                  # defining if corresponding sequence has ended\n",
    "\n",
    "    finished = tf.reduce_all(elements_finished) # -> boolean scalar\n",
    "    input = tf.cond(finished, lambda: pad_step_embedded, get_next_input)\n",
    "    state = previous_state\n",
    "    output = previous_output\n",
    "    loop_state = None\n",
    "\n",
    "    return (elements_finished, \n",
    "            input,\n",
    "            state,\n",
    "            output,\n",
    "            loop_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loop_fn(time, previous_output, previous_state, previous_loop_state):\n",
    "    if previous_state is None:    # time == 0\n",
    "        assert previous_output is None and previous_state is None\n",
    "        return loop_fn_initial()\n",
    "    else:\n",
    "        return loop_fn_transition(time, previous_output, previous_state, previous_loop_state)\n",
    "\n",
    "decoder_outputs_ta, decoder_final_state, _ = tf.nn.raw_rnn(decoder_cell, loop_fn)\n",
    "decoder_outputs = decoder_outputs_ta.stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_max_steps, decoder_batch_size, decoder_dim = tf.unstack(tf.shape(decoder_outputs))\n",
    "decoder_outputs_flat = tf.reshape(decoder_outputs, (-1, decoder_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_logits_flat = tf.add(tf.matmul(decoder_outputs_flat, W), b)\n",
    "\n",
    "decoder_logits = tf.reshape(decoder_logits_flat, (decoder_max_steps, decoder_batch_size, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_prediction = tf.argmax(decoder_logits, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stepwise_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "    labels=tf.one_hot(decoder_targets, depth=vocab_size, dtype=tf.float32),\n",
    "    logits=decoder_logits,\n",
    ")\n",
    "\n",
    "loss = tf.reduce_mean(stepwise_cross_entropy)\n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_feed(train=True):\n",
    "    if train:\n",
    "        batch = train_batches.next()\n",
    "    else:\n",
    "        batch = valid_batches.next()\n",
    "        \n",
    "    labels = make_labels(batch)\n",
    "    \n",
    "    encoder_inputs_, encoder_input_lengths_ = make_batch(batch)\n",
    "    decoder_targets_, _ = make_batch(\n",
    "        [(sequence) + [EOS] + [PAD] * 2 for sequence in labels]\n",
    "    )\n",
    "    fd = {\n",
    "        encoder_inputs: encoder_inputs_,\n",
    "        encoder_inputs_length: encoder_input_lengths_,\n",
    "    }\n",
    "    \n",
    "    if train:\n",
    "        fd[decoder_targets] = decoder_targets_\n",
    "        \n",
    "    return fd, batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _str(x):\n",
    "    return ''.join(map(id2char, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "epoch: 1/1 batch: 35150/35156 train accuracy: 0.8834\n",
      "train accuracy: 0.8834\n",
      "valid accuracy: 0.935\n",
      "  sample 1:\n",
      "    input     > early working class \n",
      "    predicted > ylrae gnikrow ssalc EPP\n",
      "\n",
      "  sample 2:\n",
      "    input     > mination of alchemic\n",
      "    predicted > noitamin fo cilehclaEPP\n",
      "\n",
      "  sample 3:\n",
      "    input     > anchorage became bot\n",
      "    predicted > egarohcna emaceb tobEPP\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1\n",
    "batches_in_epoch = int(train_size / (64 * num_unrollings))\n",
    "\n",
    "for epoch_i in range(n_epochs):\n",
    "    # train\n",
    "    print('-' * 20)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_i in range(batches_in_epoch):\n",
    "        fd, batch, labels = next_feed()\n",
    "        \n",
    "        _, l = sess.run([train_op, loss], fd)\n",
    "        predict_ = sess.run(decoder_prediction, fd)\n",
    "        \n",
    "        y_true = np.array(labels).reshape([-1,])\n",
    "        y_pred = predict_.T[:,:-3].reshape([-1,])\n",
    "        \n",
    "        correct += (y_true == y_pred).sum()\n",
    "        total += len(y_true)\n",
    "        \n",
    "        if batch_i % 10 == 0:        \n",
    "            epoch_msg = 'epoch: {}/{}'.format(epoch_i + 1, n_epochs)\n",
    "            batch_msg = 'batch: {}/{}'.format(batch_i, batches_in_epoch)\n",
    "            acc_msg = 'train accuracy: {}' .format(round(correct / total, 4))\n",
    "            \n",
    "            print('\\r' + epoch_msg + ' ' + batch_msg +' ' + acc_msg, end='')\n",
    "\n",
    "    print('\\repoch: {}/{}'.format(epoch_i + 1, n_epochs))\n",
    "    print('train accuracy:', round(correct / total, 4))\n",
    "    \n",
    "    # valid    \n",
    "    fd, batch, labels = next_feed(train=False)\n",
    "    predict_ = sess.run(decoder_prediction, fd)\n",
    "\n",
    "    y_true = np.array(labels).reshape([-1,])\n",
    "    y_pred = predict_.T[:,:-3].reshape([-1,])\n",
    "\n",
    "    correct = (y_true == y_pred).sum()\n",
    "    total = len(y_true)\n",
    "    print('valid accuracy:', correct / total)\n",
    "    \n",
    "    for i, (inp, pred) in enumerate(zip(fd[encoder_inputs].T, predict_.T)):\n",
    "        print('  sample {}:'.format(i + 1))\n",
    "        print('    input     > {}'.format(_str(inp)))\n",
    "        print('    predicted > {}'.format(_str(pred)))\n",
    "        if i >= 2:\n",
    "            break\n",
    "        print()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "--------------------\n",
    "epoch: 1/4 batch: 35150/35156 train accuracy: 0.6445\n",
    "train accuracy: 0.6445\n",
    "valid accuracy: 0.775\n",
    "  sample 1:\n",
    "    input     >  anarchism originate\n",
    "    predicted >  miihacsaa gnaaiitroEPP\n",
    "\n",
    "  sample 2:\n",
    "    input     > e magnus and aquinas\n",
    "    predicted > e snnnam dna siiiiuaEPP\n",
    "\n",
    "  sample 3:\n",
    "    input     > artially by suburban\n",
    "    predicted > yllaitra yb aarrbuusEPP\n",
    "    \n",
    "--------------------\n",
    "epoch: 2/4 batch: 35150/35156 train accuracy: 0.8222\n",
    "train accuracy: 0.8222\n",
    "valid accuracy: 0.865\n",
    "  sample 1:\n",
    "    input     > d as a term of abuse\n",
    "    predicted > d sa a mret fo esubaEPP\n",
    "\n",
    "  sample 2:\n",
    "    input     >  were among the firs\n",
    "    predicted >  erew gnoma eht srifEPP\n",
    "\n",
    "  sample 3:\n",
    "    input     >  expansion in januar\n",
    "    predicted >  noisapxxe no auunajEPP\n",
    "    \n",
    "--------------------\n",
    "epoch: 3/4 batch: 35150/35156 train accuracy: 0.8628\n",
    "train accuracy: 0.8628\n",
    "valid accuracy: 0.87\n",
    "  sample 1:\n",
    "    input     >  first used against \n",
    "    predicted >  tsrif sesu tsnaaga EPP\n",
    "\n",
    "  sample 2:\n",
    "    input     > t to take up the exa\n",
    "    predicted > t ot ekap se ott axeEPP\n",
    "\n",
    "  sample 3:\n",
    "    input     > y one nine six four \n",
    "    predicted > y eno enin xis ruof EPP\n",
    "    \n",
    "--------------------\n",
    "epoch: 4/4 batch: 35150/35156 train accuracy: 0.8834\n",
    "train accuracy: 0.8834\n",
    "valid accuracy: 0.935\n",
    "  sample 1:\n",
    "    input     > early working class \n",
    "    predicted > ylrae gnikrow ssalc EPP\n",
    "\n",
    "  sample 2:\n",
    "    input     > mination of alchemic\n",
    "    predicted > noitamin fo cilehclaEPP\n",
    "\n",
    "  sample 3:\n",
    "    input     > anchorage became bot\n",
    "    predicted > egarohcna emaceb tobEPP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he diggers of the en\n",
      "ye sreggid fo eht neEPP\n"
     ]
    }
   ],
   "source": [
    "text = valid_text[100:120]\n",
    "chars = list(map(char2id, text))\n",
    "batch = [chars]\n",
    "\n",
    "encoder_inputs_, encoder_input_lengths_ = make_batch(batch)\n",
    "\n",
    "fd = {\n",
    "    encoder_inputs: encoder_inputs_,\n",
    "    encoder_inputs_length: encoder_input_lengths_,\n",
    "}\n",
    "\n",
    "predict_ = sess.run(decoder_prediction, fd)\n",
    "\n",
    "print(text)\n",
    "print(_str(predict_.T[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
